{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3fda308",
   "metadata": {},
   "source": [
    "# Read the given CSV file in a Hive table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4d716",
   "metadata": {},
   "source": [
    "/gs://boston_mkce/data/boston.csv/\n",
    "\n",
    "/Create External table for boston/\n",
    "\n",
    "CREATE EXTERNAL TABLE boston2 (crim float, zn float,indus float,chas float,nox float,rm float,age float,dis float,rad float,tax float,pt float,b float,lstate float,mv float)\n",
    "\n",
    "ROW FORMAT DELIMITED\n",
    "\n",
    "FIELDS TERMINATED BY ','\n",
    "\n",
    "LINES TERMINATED BY '\\n'\n",
    "\n",
    "STORED AS TEXTFILE\n",
    "\n",
    "LOCATION 'gs://boston_mkce/data/';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fdb752",
   "metadata": {},
   "source": [
    "The above code is used to import the dataset to hive using cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ae80701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import seaborn as sns\n",
    "# pyspark \n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "# import linear model\n",
    "from sklearn import linear_model\n",
    "import scipy.stats as stats\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b128e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of spark class \n",
    "spark=SparkSession.builder.appName('boston').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "792dedef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-EIDTI6J:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>boston</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x200962f2d00>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ba29509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa5c97",
   "metadata": {},
   "source": [
    "# Read the data from Hive table as spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e27bc0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CRIM=0.00632, ZN=18.0, INDUS=2.309999943, CHAS=0, NOX=0.537999988, RM=6.574999809, AGE=65.19999695, DIS=4.090000153, RAD=1, TAX=296, PT=15.30000019, B=396.8999939, LSTAT=4.980000019, MV=24.0),\n",
       " Row(CRIM=0.027310001, ZN=0.0, INDUS=7.070000172, CHAS=0, NOX=0.469000012, RM=6.421000004, AGE=78.90000153, DIS=4.967100143, RAD=2, TAX=242, PT=17.79999924, B=396.8999939, LSTAT=9.140000343, MV=21.60000038)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create spark dataframe of input csv file \n",
    "df=spark.read.csv(\"C:/Users/USER/Downloads/BigDataHadoop_SparkExam (2)/boston.csv\",inferSchema=True,header=True) \n",
    "df.head(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99cb2707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64828a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, CRIM: string, ZN: string, INDUS: string, CHAS: string, NOX: string, RM: string, AGE: string, DIS: string, RAD: string, TAX: string, PT: string, B: string, LSTAT: string, MV: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "446cba93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CRIM: double (nullable = true)\n",
      " |-- ZN: double (nullable = true)\n",
      " |-- INDUS: double (nullable = true)\n",
      " |-- CHAS: integer (nullable = true)\n",
      " |-- NOX: double (nullable = true)\n",
      " |-- RM: double (nullable = true)\n",
      " |-- AGE: double (nullable = true)\n",
      " |-- DIS: double (nullable = true)\n",
      " |-- RAD: integer (nullable = true)\n",
      " |-- TAX: integer (nullable = true)\n",
      " |-- PT: double (nullable = true)\n",
      " |-- B: double (nullable = true)\n",
      " |-- LSTAT: double (nullable = true)\n",
      " |-- MV: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed34c92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column names : ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PT', 'B', 'LSTAT', 'MV']\n",
      "\n",
      "Number of Rows : 506\n",
      "\n",
      "Number of features : 14\n"
     ]
    }
   ],
   "source": [
    "# Column Names\n",
    "print(\"\\nColumn names :\", df.columns)\n",
    "# Row Count\n",
    "print(\"\\nNumber of Rows :\",df.count()) \n",
    "# Column Count\n",
    "print(\"\\nNumber of features :\", len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54ab7df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop('NOX')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732d215",
   "metadata": {},
   "source": [
    "# Get the correlation between dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2cbabe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to MV for  CRIM -0.3883046116575088\n",
      "Correlation to MV for  ZN 0.36044534463752903\n",
      "Correlation to MV for  INDUS -0.48372517128143383\n",
      "Correlation to MV for  CHAS 0.17526017775291847\n",
      "Correlation to MV for  NOX -0.4273207763683772\n",
      "Correlation to MV for  RM 0.695359937127267\n",
      "Correlation to MV for  AGE -0.37695456714288667\n",
      "Correlation to MV for  DIS 0.24992873873512172\n",
      "Correlation to MV for  RAD -0.3816262315669168\n",
      "Correlation to MV for  TAX -0.46853593528654536\n",
      "Correlation to MV for  PT -0.5077867038116085\n",
      "Correlation to MV for  B 0.3334608226834164\n",
      "Correlation to MV for  LSTAT -0.7376627294671615\n",
      "Correlation to MV for  MV 1.0\n"
     ]
    }
   ],
   "source": [
    "import six\n",
    "for i in df.columns:\n",
    "    if not( isinstance(df.select(i).take(1)[0][0], six.string_types)):\n",
    "        print( \"Correlation to MV for \", i, df.stat.corr('MV',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ea12e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PT', 'B', 'LSTAT'], outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e290c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vhouse_df = vectorAssembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bafa77cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|            features|         MV|\n",
      "+--------------------+-----------+\n",
      "|[0.00632,18.0,2.3...|       24.0|\n",
      "|[0.027310001,0.0,...|21.60000038|\n",
      "|[0.02729,0.0,7.07...|34.70000076|\n",
      "+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vhouse_df =vhouse_df.select(['features', 'MV'])\n",
    "vhouse_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108bda5d",
   "metadata": {},
   "source": [
    "# Build a linear regression model to predict house price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5765a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vhouse_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7591fe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.057884278993582644,0.024627352989761615,-0.03297376922363685,1.6041807383501399,-10.046504003008234,4.9155996062835055,0.0,-0.8189660256118617,0.0,-0.001352806748156607,-0.7136391819689502,0.00585861576108996,-0.3809046130693966]\n",
      "Intercept: 16.971596906851424\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='MV', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de92d8",
   "metadata": {},
   "source": [
    "# Evaluate the Linear Regression model by getting the RMSE and R-squared values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "974ef34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Value : 4.826398\n",
      "R-Squared Value : 0.728430\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE Value : %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"R-Squared Value : %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d20a597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|                MV|\n",
      "+-------+------------------+\n",
      "|  count|               349|\n",
      "|   mean|22.594842436659025|\n",
      "| stddev| 9.274809981969723|\n",
      "|    min|               5.0|\n",
      "|    max|              50.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33d982a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|         MV|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "| 30.01309945866215|       24.0|[0.00632,18.0,2.3...|\n",
      "| 41.98969616280249|       50.0|[0.01501,90.0,1.2...|\n",
      "| 26.29585081205599|30.10000038|[0.01709,90.0,2.0...|\n",
      "|26.284540202946197|23.10000038|[0.0187,85.0,4.15...|\n",
      "|26.633572811896702|       25.0|[0.028750001,28.0...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.678908\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"MV\",\"features\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"MV\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fd52f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5.11172\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb679c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 10\n",
      "objectiveHistory: [0.49999999999999956, 0.43011532360272786, 0.22967057742208366, 0.2072437505762283, 0.17935380550381416, 0.17637624857836237, 0.17595774340477083, 0.17530356904605493, 0.1748087682376762, 0.1742553053995719, 0.17423947292439113]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  0.2534795558068481|\n",
      "|  -5.932266351735606|\n",
      "|  0.7519585969684357|\n",
      "|   4.383133687931185|\n",
      "|  0.3889921931520455|\n",
      "|  10.672321655050354|\n",
      "|-0.02286953150970...|\n",
      "| -1.6394356470214646|\n",
      "| -3.2840586053362877|\n",
      "|   8.041120291657705|\n",
      "|  1.0013719105655028|\n",
      "|   6.785565344286184|\n",
      "|  -1.857095524134742|\n",
      "|   9.260673945570232|\n",
      "|  -1.430282982550935|\n",
      "|   5.090851352243739|\n",
      "| -0.4525356715836075|\n",
      "|  -9.291438215789999|\n",
      "|  -4.187742369687125|\n",
      "|  3.6879837886163784|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "839c6d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|         MV|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "| 30.01309945866215|       24.0|[0.00632,18.0,2.3...|\n",
      "| 41.98969616280249|       50.0|[0.01501,90.0,1.2...|\n",
      "| 26.29585081205599|30.10000038|[0.01709,90.0,2.0...|\n",
      "|26.284540202946197|23.10000038|[0.0187,85.0,4.15...|\n",
      "|26.633572811896702|       25.0|[0.028750001,28.0...|\n",
      "| 29.28848507768263|31.20000076|[0.03049,55.0,3.7...|\n",
      "|19.739821781947786|       17.5|[0.031129999,0.0,...|\n",
      "| 27.91750820477462|       22.0|[0.03537,34.0,6.0...|\n",
      "|22.344271222752344|20.70000076|[0.037379999,0.0,...|\n",
      "|27.010437382698406|       22.0|[0.03932,0.0,3.41...|\n",
      "| 35.99897442192811|33.29999924|[0.040109999,80.0...|\n",
      "| 27.01781019277925|22.89999962|[0.042029999,28.0...|\n",
      "|25.196207302052464|20.60000038|[0.042939998,28.0...|\n",
      "|24.340936714568013|19.39999962|[0.043790001,80.0...|\n",
      "|30.896516408606033|24.79999924|[0.04417,70.0,2.2...|\n",
      "|33.138769632421024|30.29999924|[0.046659999,80.0...|\n",
      "| 22.37640237422969|11.89999962|[0.04741,0.0,11.9...|\n",
      "|30.053819419437815|28.20000076|[0.049320001,33.0...|\n",
      "| 24.92584957648529|23.89999962|[0.050590001,0.0,...|\n",
      "|22.136317931678818|       22.5|[0.051879998,0.0,...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"MV\",\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29dda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
